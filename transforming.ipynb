{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo-0301\", temperature=0):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRateLimitError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_1160/3552055338.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mHi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mI\u001B[0m \u001B[0mwould\u001B[0m \u001B[0mlike\u001B[0m \u001B[0mto\u001B[0m \u001B[0morder\u001B[0m \u001B[0ma\u001B[0m \u001B[0mblender\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \"\"\"\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mresponse\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_completion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprompt\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresponse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_1160/1452302787.py\u001B[0m in \u001B[0;36mget_completion\u001B[1;34m(prompt, model, temperature)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mget_completion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprompt\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"gpt-3.5-turbo-0301\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtemperature\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m     \u001B[0mmessages\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m\"role\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m\"user\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"content\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mprompt\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m     response = openai.ChatCompletion.create(\n\u001B[0m\u001B[0;32m      4\u001B[0m         \u001B[0mmodel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m         \u001B[0mmessages\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmessages\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\u001B[0m in \u001B[0;36mcreate\u001B[1;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[0;32m     23\u001B[0m         \u001B[1;32mwhile\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 25\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcreate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     26\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mTryAgain\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[0mstart\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\u001B[0m in \u001B[0;36mcreate\u001B[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[0;32m    151\u001B[0m         )\n\u001B[0;32m    152\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 153\u001B[1;33m         response, _, api_key = requestor.request(\n\u001B[0m\u001B[0;32m    154\u001B[0m             \u001B[1;34m\"post\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    155\u001B[0m             \u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001B[0m in \u001B[0;36mrequest\u001B[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[0;32m    228\u001B[0m             \u001B[0mrequest_timeout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrequest_timeout\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    229\u001B[0m         )\n\u001B[1;32m--> 230\u001B[1;33m         \u001B[0mresp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgot_stream\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_interpret_response\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstream\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    231\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mresp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgot_stream\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapi_key\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    232\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001B[0m in \u001B[0;36m_interpret_response\u001B[1;34m(self, result, stream)\u001B[0m\n\u001B[0;32m    622\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    623\u001B[0m             return (\n\u001B[1;32m--> 624\u001B[1;33m                 self._interpret_response_line(\n\u001B[0m\u001B[0;32m    625\u001B[0m                     \u001B[0mresult\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcontent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdecode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"utf-8\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    626\u001B[0m                     \u001B[0mresult\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstatus_code\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001B[0m in \u001B[0;36m_interpret_response_line\u001B[1;34m(self, rbody, rcode, rheaders, stream)\u001B[0m\n\u001B[0;32m    685\u001B[0m         \u001B[0mstream_error\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstream\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;34m\"error\"\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mresp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    686\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mstream_error\u001B[0m \u001B[1;32mor\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;36m200\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[0mrcode\u001B[0m \u001B[1;33m<\u001B[0m \u001B[1;36m300\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 687\u001B[1;33m             raise self.handle_error_response(\n\u001B[0m\u001B[0;32m    688\u001B[0m                 \u001B[0mrbody\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrcode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrheaders\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstream_error\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstream_error\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    689\u001B[0m             )\n",
      "\u001B[1;31mRateLimitError\u001B[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Translate the following English text to Spanish: \\\n",
    "```Hi, I would like to order a blender```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Tell me which language this is:\n",
    "```Combien coûte le lampadaire?```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Translate the following  text to French and Spanish\n",
    "and English pirate: \\\n",
    "```I want to order a basketball```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Translate the following text to Spanish in both the \\\n",
    "formal and informal forms:\n",
    "'Would you like to order a pillow?'\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_messages = [\n",
    "  \"La performance du système est plus lente que d'habitude.\",  # System performance is slower than normal\n",
    "  \"Mi monitor tiene píxeles que no se iluminan.\",              # My monitor has pixels that are not lighting\n",
    "  \"Il mio mouse non funziona\",                                 # My mouse is not working\n",
    "  \"Mój klawisz Ctrl jest zepsuty\",                             # My keyboard has a broken control key\n",
    "  \"我的屏幕在闪烁\"                                               # My screen is flashing\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for issue in user_messages:\n",
    "    prompt = f\"Tell me what language this is: ```{issue}```\"\n",
    "    lang = get_completion(prompt)\n",
    "    print(f\"Original message ({lang}): {issue}\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Translate the following  text to English \\\n",
    "    and Korean: ```{issue}```\n",
    "    \"\"\"\n",
    "    response = get_completion(prompt)\n",
    "    print(response, \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Translate the following from slang to a business letter:\n",
    "'Dude, This is Joe, check out this spec on this standing lamp.'\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_json = { \"resturant employees\" :[\n",
    "    {\"name\":\"Shyam\", \"email\":\"shyamjaiswal@gmail.com\"},\n",
    "    {\"name\":\"Bob\", \"email\":\"bob32@gmail.com\"},\n",
    "    {\"name\":\"Jai\", \"email\":\"jai87@gmail.com\"}\n",
    "]}\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Translate the following python dictionary from JSON to an HTML \\\n",
    "table with column headers and title: {data_json}\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = [\n",
    "  \"The girl with the black and white puppies have a ball.\",  # The girl has a ball.\n",
    "  \"Yolanda has her notebook.\", # ok\n",
    "  \"Its going to be a long day. Does the car need it’s oil changed?\",  # Homonyms\n",
    "  \"Their goes my freedom. There going to bring they’re suitcases.\",  # Homonyms\n",
    "  \"Your going to need you’re notebook.\",  # Homonyms\n",
    "  \"That medicine effects my ability to sleep. Have you heard of the butterfly affect?\", # Homonyms\n",
    "  \"This phrase is to cherck chatGPT for speling abilitty\"  # spelling\n",
    "]\n",
    "for t in text:\n",
    "    prompt = f\"\"\"Proofread and correct the following text\n",
    "    and rewrite the corrected version. If you don't find\n",
    "    and errors, just say \"No errors found\". Don't use\n",
    "    any punctuation around the text:\n",
    "    ```{t}```\"\"\"\n",
    "    response = get_completion(prompt)\n",
    "    print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "Got this for my daughter for her birthday cuz she keeps taking \\\n",
    "mine from my room.  Yes, adults also like pandas too.  She takes \\\n",
    "it everywhere with her, and it's super soft and cute.  One of the \\\n",
    "ears is a bit lower than the other, and I don't think that was \\\n",
    "designed to be asymmetrical. It's a bit small for what I paid for it \\\n",
    "though. I think there might be other options that are bigger for \\\n",
    "the same price.  It arrived a day earlier than expected, so I got \\\n",
    "to play with it myself before I gave it to my daughter.\n",
    "\"\"\"\n",
    "prompt = f\"proofread and correct this review: ```{text}```\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from redlines import Redlines\n",
    "\n",
    "diff = Redlines(text,response)\n",
    "display(Markdown(diff.output_markdown))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "proofread and correct this review. Make it more compelling.\n",
    "Ensure it follows APA style guide and targets an advanced reader.\n",
    "Output in markdown format.\n",
    "Text: ```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "display(Markdown(response))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}